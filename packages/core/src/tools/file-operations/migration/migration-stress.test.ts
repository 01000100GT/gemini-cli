/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import { describe, it, expect, beforeEach, afterEach, vi } from 'vitest';
import * as fs from 'node:fs';
import * as path from 'node:path';
import * as os from 'node:os';
import { Config } from '../../../config/config.js';
import { 
  FileOperationsMigrationConfig, 
  MigrationPhase, 
  MIGRATION_PRESETS 
} from './migration-config.js';
import { MigrationAwareToolRegistry } from './migration-tool-registry.js';
import { UsageMetricsCollector } from './usage-metrics.js';
import { MigrationManager } from './migration-manager.js';

/**
 * Stress tests for FileOperations migration under high load
 */
describe('FileOperations Migration Stress Tests', () => {\n  let tempDir: string;\n  let originalConsoleLog: typeof console.log;\n  \n  beforeEach(async () => {\n    tempDir = fs.mkdtempSync(path.join(os.tmpdir(), 'migration-stress-'));\n    originalConsoleLog = console.log;\n    console.log = vi.fn();\n  });\n  \n  afterEach(() => {\n    console.log = originalConsoleLog;\n    if (fs.existsSync(tempDir)) {\n      fs.rmSync(tempDir, { recursive: true, force: true });\n    }\n  });\n  \n  describe('High Volume Operations', () => {\n    it('should handle thousands of file operations without degradation', async () => {\n      const config = createTestConfig(tempDir, {\n        ...MIGRATION_PRESETS.DEVELOPMENT,\n        features: {\n          predictiveCaching: true,\n          parallelExecution: true,\n          transactionManagement: true,\n          securityHardening: true,\n          advancedAnalytics: false,\n        },\n      });\n      \n      const registry = await config.getToolRegistry();\n      const readTool = registry.getTool('ReadFileTool');\n      const writeTool = registry.getTool('WriteFileTool');\n      \n      expect(readTool).toBeDefined();\n      expect(writeTool).toBeDefined();\n      \n      // Create test files\n      const numFiles = 1000;\n      const testFiles: string[] = [];\n      \n      console.log(`Creating ${numFiles} test files...`);\n      for (let i = 0; i < numFiles; i++) {\n        const filePath = path.join(tempDir, `stress-test-${i}.txt`);\n        const content = `Stress test file ${i}\\nContent for performance testing\\n`.repeat(10);\n        fs.writeFileSync(filePath, content);\n        testFiles.push(filePath);\n      }\n      \n      // Measure read performance\n      console.log('Starting read operations...');\n      const readStartTime = performance.now();\n      \n      const readPromises = testFiles.map(async (filePath) => {\n        return await readTool!.execute({ path: filePath });\n      });\n      \n      const readResults = await Promise.all(readPromises);\n      const readEndTime = performance.now();\n      const readDuration = readEndTime - readStartTime;\n      \n      expect(readResults).toHaveLength(numFiles);\n      readResults.forEach((result, index) => {\n        expect(result.content).toContain(`Stress test file ${index}`);\n      });\n      \n      console.log(`Read ${numFiles} files in ${readDuration.toFixed(2)}ms`);\n      console.log(`Average read time: ${(readDuration / numFiles).toFixed(2)}ms per file`);\n      \n      // Performance threshold: should read files at reasonable speed\n      const avgReadTime = readDuration / numFiles;\n      expect(avgReadTime).toBeLessThan(50); // Less than 50ms per file on average\n      \n      // Measure write performance\n      console.log('Starting write operations...');\n      const writeStartTime = performance.now();\n      \n      const writePromises = testFiles.map(async (filePath, index) => {\n        const newContent = `Updated stress test file ${index}\\nModified content\\n`;\n        return await writeTool!.execute({ path: filePath, content: newContent });\n      });\n      \n      const writeResults = await Promise.all(writePromises);\n      const writeEndTime = performance.now();\n      const writeDuration = writeEndTime - writeStartTime;\n      \n      expect(writeResults).toHaveLength(numFiles);\n      \n      console.log(`Wrote ${numFiles} files in ${writeDuration.toFixed(2)}ms`);\n      console.log(`Average write time: ${(writeDuration / numFiles).toFixed(2)}ms per file`);\n      \n      // Performance threshold: should write files at reasonable speed\n      const avgWriteTime = writeDuration / numFiles;\n      expect(avgWriteTime).toBeLessThan(100); // Less than 100ms per file on average\n    }, 60000); // 60 second timeout\n    \n    it('should handle rapid sequential operations', async () => {\n      const config = createTestConfig(tempDir, MIGRATION_PRESETS.DEVELOPMENT);\n      const registry = await config.getToolRegistry();\n      \n      const testFile = path.join(tempDir, 'rapid-operations.txt');\n      const readTool = registry.getTool('ReadFileTool');\n      const writeTool = registry.getTool('WriteFileTool');\n      \n      // Initialize file\n      fs.writeFileSync(testFile, 'Initial content');\n      \n      // Perform rapid operations\n      const numOperations = 100;\n      const startTime = performance.now();\n      \n      for (let i = 0; i < numOperations; i++) {\n        // Read\n        const readResult = await readTool!.execute({ path: testFile });\n        expect(readResult.content).toBeTruthy();\n        \n        // Write\n        const newContent = `Rapid operation ${i}\\n${readResult.content}`;\n        await writeTool!.execute({ path: testFile, content: newContent });\n      }\n      \n      const endTime = performance.now();\n      const duration = endTime - startTime;\n      \n      console.log(`Completed ${numOperations * 2} operations in ${duration.toFixed(2)}ms`);\n      console.log(`Average operation time: ${(duration / (numOperations * 2)).toFixed(2)}ms`);\n      \n      // Verify final state\n      const finalContent = fs.readFileSync(testFile, 'utf-8');\n      expect(finalContent).toContain('Rapid operation 99');\n      \n      // Performance should be reasonable\n      const avgOpTime = duration / (numOperations * 2);\n      expect(avgOpTime).toBeLessThan(50); // Less than 50ms per operation\n    });\n    \n    it('should handle large file operations efficiently', async () => {\n      const config = createTestConfig(tempDir, {\n        ...MIGRATION_PRESETS.DEVELOPMENT,\n        features: {\n          predictiveCaching: true,\n          parallelExecution: true,\n          transactionManagement: true,\n          securityHardening: false, // Disable for performance\n          advancedAnalytics: false,\n        },\n      });\n      \n      const registry = await config.getToolRegistry();\n      const readTool = registry.getTool('ReadFileTool');\n      const writeTool = registry.getTool('WriteFileTool');\n      \n      // Create large files\n      const largeSizes = [1, 5, 10]; // MB\n      const largeFiles: Array<{ path: string; size: number }> = [];\n      \n      for (const sizeMB of largeSizes) {\n        const filePath = path.join(tempDir, `large-file-${sizeMB}MB.txt`);\n        const content = 'x'.repeat(sizeMB * 1024 * 1024); // sizeMB worth of 'x'\n        \n        console.log(`Creating ${sizeMB}MB file...`);\n        const writeStart = performance.now();\n        await writeTool!.execute({ path: filePath, content });\n        const writeEnd = performance.now();\n        \n        largeFiles.push({ path: filePath, size: sizeMB });\n        \n        console.log(`Created ${sizeMB}MB file in ${(writeEnd - writeStart).toFixed(2)}ms`);\n        \n        // Verify file was created correctly\n        const stats = fs.statSync(filePath);\n        expect(stats.size).toBe(sizeMB * 1024 * 1024);\n      }\n      \n      // Read large files\n      for (const file of largeFiles) {\n        console.log(`Reading ${file.size}MB file...`);\n        const readStart = performance.now();\n        \n        const result = await readTool!.execute({ path: file.path });\n        \n        const readEnd = performance.now();\n        const readDuration = readEnd - readStart;\n        \n        expect(result.content.length).toBe(file.size * 1024 * 1024);\n        \n        console.log(`Read ${file.size}MB file in ${readDuration.toFixed(2)}ms`);\n        \n        // Performance should scale reasonably with file size\n        const throughputMBps = file.size / (readDuration / 1000);\n        expect(throughputMBps).toBeGreaterThan(10); // At least 10 MB/s\n        \n        console.log(`Throughput: ${throughputMBps.toFixed(2)} MB/s`);\n      }\n    }, 120000); // 2 minute timeout for large files\n  });\n  \n  describe('Concurrent User Simulation', () => {\n    it('should handle multiple concurrent users', async () => {\n      const config = createTestConfig(tempDir, MIGRATION_PRESETS.DEVELOPMENT);\n      const registry = await config.getToolRegistry() as MigrationAwareToolRegistry;\n      \n      const numUsers = 50;\n      const operationsPerUser = 20;\n      \n      // Create user-specific test files\n      const userFiles: Array<{ userId: string; files: string[] }> = [];\n      \n      for (let userId = 0; userId < numUsers; userId++) {\n        const userDir = path.join(tempDir, `user-${userId}`);\n        fs.mkdirSync(userDir);\n        \n        const files: string[] = [];\n        for (let fileId = 0; fileId < 5; fileId++) {\n          const filePath = path.join(userDir, `file-${fileId}.txt`);\n          fs.writeFileSync(filePath, `User ${userId} file ${fileId}`);\n          files.push(filePath);\n        }\n        \n        userFiles.push({ userId: `user-${userId}`, files });\n      }\n      \n      // Simulate concurrent users\n      console.log(`Simulating ${numUsers} concurrent users with ${operationsPerUser} operations each...`);\n      const startTime = performance.now();\n      \n      const userPromises = userFiles.map(async (userData) => {\n        // Set user context\n        registry.setUserContext({ userId: userData.userId });\n        \n        const readTool = registry.getTool('ReadFileTool');\n        const writeTool = registry.getTool('WriteFileTool');\n        \n        // Perform operations for this user\n        const operations = [];\n        \n        for (let op = 0; op < operationsPerUser; op++) {\n          const randomFile = userData.files[Math.floor(Math.random() * userData.files.length)];\n          \n          if (Math.random() > 0.5) {\n            // Read operation\n            operations.push(\n              readTool!.execute({ path: randomFile })\n            );\n          } else {\n            // Write operation\n            operations.push(\n              writeTool!.execute({ \n                path: randomFile, \n                content: `${userData.userId} operation ${op}` \n              })\n            );\n          }\n        }\n        \n        return Promise.all(operations);\n      });\n      \n      const allResults = await Promise.all(userPromises);\n      const endTime = performance.now();\n      const duration = endTime - startTime;\n      \n      const totalOperations = numUsers * operationsPerUser;\n      console.log(`Completed ${totalOperations} operations from ${numUsers} users in ${duration.toFixed(2)}ms`);\n      console.log(`Average operation time: ${(duration / totalOperations).toFixed(2)}ms`);\n      \n      // Verify all operations completed\n      expect(allResults).toHaveLength(numUsers);\n      allResults.forEach((userResults, index) => {\n        expect(userResults).toHaveLength(operationsPerUser);\n      });\n      \n      // Performance should be reasonable under concurrent load\n      const avgOpTime = duration / totalOperations;\n      expect(avgOpTime).toBeLessThan(200); // Less than 200ms per operation under load\n    }, 180000); // 3 minute timeout\n    \n    it('should handle migration rollout under load', async () => {\n      const rolloutSteps = [0, 25, 50, 75, 100];\n      const usersPerStep = 20;\n      const operationsPerUser = 10;\n      \n      for (const rolloutPercentage of rolloutSteps) {\n        console.log(`Testing rollout at ${rolloutPercentage}%...`);\n        \n        const migrationConfig: FileOperationsMigrationConfig = {\n          ...MIGRATION_PRESETS.DEVELOPMENT,\n          rolloutPercentage,\n        };\n        \n        const config = createTestConfig(tempDir, migrationConfig);\n        const registry = await config.getToolRegistry() as MigrationAwareToolRegistry;\n        \n        const migrationMetrics = registry.getMigrationMetrics();\n        const usageCollector = new UsageMetricsCollector(migrationMetrics);\n        \n        // Simulate users at this rollout percentage\n        const stepStartTime = performance.now();\n        \n        const userPromises = Array.from({ length: usersPerStep }, async (_, userIndex) => {\n          const userId = `rollout-user-${rolloutPercentage}-${userIndex}`;\n          registry.setUserContext({ userId });\n          \n          const readTool = registry.getTool('ReadFileTool');\n          const testFile = path.join(tempDir, `rollout-test-${userId}.txt`);\n          \n          // Create test file if it doesn't exist\n          if (!fs.existsSync(testFile)) {\n            fs.writeFileSync(testFile, `Test content for ${userId}`);\n          }\n          \n          // Perform operations\n          const operations = [];\n          for (let op = 0; op < operationsPerUser; op++) {\n            operations.push(\n              readTool!.execute({ path: testFile })\n            );\n          }\n          \n          return Promise.all(operations);\n        });\n        \n        const stepResults = await Promise.all(userPromises);\n        const stepEndTime = performance.now();\n        const stepDuration = stepEndTime - stepStartTime;\n        \n        const stepTotalOps = usersPerStep * operationsPerUser;\n        console.log(`${rolloutPercentage}% rollout: ${stepTotalOps} operations in ${stepDuration.toFixed(2)}ms`);\n        \n        // Verify all operations completed\n        expect(stepResults).toHaveLength(usersPerStep);\n        \n        // Generate usage comparison\n        const comparison = usageCollector.generateToolComparison('ReadFileTool', 1);\n        if (comparison) {\n          console.log(`Adoption rate: ${(comparison.comparison.adoptionRate * 100).toFixed(1)}%`);\n          \n          // Adoption rate should roughly match rollout percentage\n          if (rolloutPercentage > 0) {\n            expect(comparison.comparison.adoptionRate).toBeGreaterThan(0);\n          }\n        }\n      }\n    }, 300000); // 5 minute timeout\n  });\n  \n  describe('Memory and Resource Management', () => {\n    it('should not leak memory during extended operations', async () => {\n      const config = createTestConfig(tempDir, MIGRATION_PRESETS.DEVELOPMENT);\n      const registry = await config.getToolRegistry();\n      \n      const readTool = registry.getTool('ReadFileTool');\n      const testFile = path.join(tempDir, 'memory-test.txt');\n      \n      // Create test file\n      const testContent = 'Memory test content\\n'.repeat(1000);\n      fs.writeFileSync(testFile, testContent);\n      \n      // Get initial memory usage\n      const initialMemory = process.memoryUsage();\n      console.log('Initial memory:', {\n        rss: `${(initialMemory.rss / 1024 / 1024).toFixed(2)} MB`,\n        heapUsed: `${(initialMemory.heapUsed / 1024 / 1024).toFixed(2)} MB`,\n      });\n      \n      // Perform many operations\n      const numOperations = 1000;\n      console.log(`Performing ${numOperations} read operations...`);\n      \n      for (let i = 0; i < numOperations; i++) {\n        const result = await readTool!.execute({ path: testFile });\n        expect(result.content).toBe(testContent);\n        \n        // Force garbage collection periodically if available\n        if (global.gc && i % 100 === 0) {\n          global.gc();\n        }\n      }\n      \n      // Check final memory usage\n      const finalMemory = process.memoryUsage();\n      console.log('Final memory:', {\n        rss: `${(finalMemory.rss / 1024 / 1024).toFixed(2)} MB`,\n        heapUsed: `${(finalMemory.heapUsed / 1024 / 1024).toFixed(2)} MB`,\n      });\n      \n      // Memory growth should be reasonable\n      const memoryGrowthMB = (finalMemory.heapUsed - initialMemory.heapUsed) / 1024 / 1024;\n      console.log(`Memory growth: ${memoryGrowthMB.toFixed(2)} MB`);\n      \n      // Should not grow more than 50MB for 1000 operations\n      expect(memoryGrowthMB).toBeLessThan(50);\n    });\n    \n    it('should handle file descriptor limits gracefully', async () => {\n      const config = createTestConfig(tempDir, {\n        ...MIGRATION_PRESETS.DEVELOPMENT,\n        features: {\n          predictiveCaching: false, // Disable caching to test raw file operations\n          parallelExecution: true,\n          transactionManagement: true,\n          securityHardening: true,\n          advancedAnalytics: false,\n        },\n      });\n      \n      const registry = await config.getToolRegistry();\n      const readTool = registry.getTool('ReadFileTool');\n      \n      // Create many test files\n      const numFiles = 500; // Approach typical file descriptor limits\n      const testFiles: string[] = [];\n      \n      for (let i = 0; i < numFiles; i++) {\n        const filePath = path.join(tempDir, `fd-test-${i}.txt`);\n        fs.writeFileSync(filePath, `File descriptor test ${i}`);\n        testFiles.push(filePath);\n      }\n      \n      console.log(`Testing with ${numFiles} files...`);\n      \n      // Read all files concurrently (this could exhaust file descriptors)\n      const startTime = performance.now();\n      \n      try {\n        const readPromises = testFiles.map(filePath => \n          readTool!.execute({ path: filePath })\n        );\n        \n        const results = await Promise.all(readPromises);\n        \n        const endTime = performance.now();\n        console.log(`Successfully read ${numFiles} files in ${(endTime - startTime).toFixed(2)}ms`);\n        \n        // Verify all reads succeeded\n        expect(results).toHaveLength(numFiles);\n        results.forEach((result, index) => {\n          expect(result.content).toContain(`File descriptor test ${index}`);\n        });\n        \n      } catch (error) {\n        // If we get EMFILE (too many open files), that's expected at some point\n        if ((error as any).code === 'EMFILE') {\n          console.log('Reached file descriptor limit (expected)');\n        } else {\n          throw error;\n        }\n      }\n    });\n  });\n  \n  describe('Error Recovery Under Load', () => {\n    it('should handle partial failures gracefully', async () => {\n      const config = createTestConfig(tempDir, MIGRATION_PRESETS.DEVELOPMENT);\n      const registry = await config.getToolRegistry();\n      \n      const readTool = registry.getTool('ReadFileTool');\n      const writeTool = registry.getTool('WriteFileTool');\n      \n      // Create mix of valid and invalid file paths\n      const validFiles: string[] = [];\n      const invalidFiles: string[] = [];\n      \n      for (let i = 0; i < 50; i++) {\n        // Valid files\n        const validPath = path.join(tempDir, `valid-${i}.txt`);\n        fs.writeFileSync(validPath, `Valid file ${i}`);\n        validFiles.push(validPath);\n        \n        // Invalid files (non-existent)\n        const invalidPath = path.join(tempDir, `invalid-${i}.txt`);\n        invalidFiles.push(invalidPath);\n      }\n      \n      // Mix valid and invalid operations\n      const allPaths = [...validFiles, ...invalidFiles].sort(() => Math.random() - 0.5);\n      \n      console.log(`Testing ${allPaths.length} operations (${validFiles.length} valid, ${invalidFiles.length} invalid)...`);\n      \n      // Execute all operations and track results\n      const results = await Promise.allSettled(\n        allPaths.map(filePath => readTool!.execute({ path: filePath }))\n      );\n      \n      const successful = results.filter(r => r.status === 'fulfilled');\n      const failed = results.filter(r => r.status === 'rejected');\n      \n      console.log(`Results: ${successful.length} successful, ${failed.length} failed`);\n      \n      // Should have roughly the right number of successes and failures\n      expect(successful.length).toBe(validFiles.length);\n      expect(failed.length).toBe(invalidFiles.length);\n      \n      // Verify successful results contain expected content\n      successful.forEach((result, index) => {\n        if (result.status === 'fulfilled') {\n          expect(result.value.content).toContain('Valid file');\n        }\n      });\n    });\n    \n    it('should recover from temporary system resource exhaustion', async () => {\n      const config = createTestConfig(tempDir, MIGRATION_PRESETS.DEVELOPMENT);\n      const registry = await config.getToolRegistry();\n      \n      const writeTool = registry.getTool('WriteFileTool');\n      const readTool = registry.getTool('ReadFileTool');\n      \n      // Create scenario that might exhaust resources\n      const resourceIntensiveOperations = [];\n      \n      for (let i = 0; i < 100; i++) {\n        const filePath = path.join(tempDir, `resource-test-${i}.txt`);\n        const largeContent = 'x'.repeat(100000); // 100KB per file\n        \n        resourceIntensiveOperations.push(\n          writeTool!.execute({ path: filePath, content: largeContent })\n        );\n      }\n      \n      console.log('Executing resource-intensive operations...');\n      const startTime = performance.now();\n      \n      try {\n        // Execute in smaller batches to avoid overwhelming the system\n        const batchSize = 10;\n        for (let i = 0; i < resourceIntensiveOperations.length; i += batchSize) {\n          const batch = resourceIntensiveOperations.slice(i, i + batchSize);\n          await Promise.all(batch);\n          \n          // Small delay between batches\n          await new Promise(resolve => setTimeout(resolve, 10));\n        }\n        \n        const endTime = performance.now();\n        console.log(`Completed resource-intensive operations in ${(endTime - startTime).toFixed(2)}ms`);\n        \n        // Verify operations completed successfully\n        const verificationOperations = [];\n        for (let i = 0; i < 100; i++) {\n          const filePath = path.join(tempDir, `resource-test-${i}.txt`);\n          verificationOperations.push(\n            readTool!.execute({ path: filePath })\n          );\n        }\n        \n        const verificationResults = await Promise.all(verificationOperations);\n        expect(verificationResults).toHaveLength(100);\n        \n        verificationResults.forEach(result => {\n          expect(result.content.length).toBe(100000);\n        });\n        \n        console.log('All operations verified successfully');\n        \n      } catch (error) {\n        console.log('Resource exhaustion occurred (may be expected):', error);\n        \n        // Even if some operations fail due to resource limits,\n        // the system should remain stable\n        expect(error).toBeDefined();\n      }\n    });\n  });\n});\n\n// Helper function\nfunction createTestConfig(\n  targetDir: string,\n  migrationConfig: FileOperationsMigrationConfig\n): Config {\n  return new Config({\n    contentGeneratorConfig: {\n      model: 'test-model',\n      apiKey: 'test-key',\n    },\n    embeddingModel: 'test-embedding',\n    targetDir,\n    debugMode: false,\n    fileOperationsMigration: migrationConfig,\n  });\n}\n